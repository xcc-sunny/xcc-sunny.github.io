[{"authors":["admin"],"categories":null,"content":"Yujie He is currently a Master student in Robotics at √âcole polytechnique f√©d√©rale de Lausanne (EPFL), Switzerland. Before that, I received my Bachelor\u0026rsquo;s Degree in Mechanical Engineering (Mechatronics Track) with Excellent Graduate Honor in 2020 from Tongji University, Shanghai, China.\n\u0026ldquo;Diversity is essential to happiness\u0026rdquo; from Philosopher Bertrand Russell is my life motto, which inspires me to explore the interdisciplinary realm of robotic intelligence through industrial internship and research exploration. Since October, I have been a research assistant at LASA, focusing on multi-modal pedestrian behavior analysis for Qolo robot of CrowdBot project under the supervision of Diego Paez-Granados. Between Feb. to Jul. 2021, I conducted the semester project on vision-based navigation for drone delivery supervised by Valentin W√ºest and Dr. Przemyslaw Kornatowski at LIS. Previously, I worked on visual tracking for unmanned aerial vehicles supervised by Prof. Changhong Fu in Vision4Robotics Group during my bachelor study.\nMy current research interests are mostly focused on incorporating cutting-edge machine learning approaches into visual perception for robots. In addition, I am collaborating closely with Prof. Peng Lu from ArcLab @ HKU and Dr. Kai Sun from Hesai Technology.\nFor more info, please refer to the latest CV here.\n1f407a Â§™Ê∑± 6dbdc7 Â§™ÊµÖ 58979f ‰∏çÈîô -- \n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yujie-he.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Yujie He is currently a Master student in Robotics at √âcole polytechnique f√©d√©rale de Lausanne (EPFL), Switzerland. Before that, I received my Bachelor\u0026rsquo;s Degree in Mechanical Engineering (Mechatronics Track) with Excellent Graduate Honor in 2020 from Tongji University, Shanghai, China.\n\u0026ldquo;Diversity is essential to happiness\u0026rdquo; from Philosopher Bertrand Russell is my life motto, which inspires me to explore the interdisciplinary realm of robotic intelligence through industrial internship and research exploration. Since October, I have been a research assistant at LASA, focusing on multi-modal pedestrian behavior analysis for Qolo robot of CrowdBot project under the supervision of Diego Paez-Granados.","tags":null,"title":"Yujie He","type":"authors"},{"authors":null,"categories":null,"content":" TJ-100685 Ê∑±Â∫¶Â≠¶‰π† | Deep learning (Elective Course)  Supervised by Professor, Yin Wang\nGitHub Repo: hibetterheyj/tju_deep_learning\n Course info The course covers the following topics:\n Python basics Linear regression \u0026amp; Logistic regression NN basics Convolutional neural network Object detection Style transfer NLP basics Applicant: Medical image processing  Final Project Enhanced 3D Zonal Segmentation of the Prostate on MRI via Enhanced Weight-Standardization and GroupNorm\nÂü∫‰∫éÁöÑWeight-StandardizationÂíåGroupNormÁöÑ‰∏âÁª¥ÂâçÂàóËÖ∫MRIÂå∫ÂùóÂàÜÂâ≤\nüí•For more details, please refer to jupyter notebook for final project.\nAssignments Using nbviewer for better and faster rendering!\n (1) Linear \u0026amp; (2) Logistic (multiple binary classifiers method) regression  [notebook1], [notebook2]\n (1) Logistic (softmax method) regression \u0026amp; (2) Fully connected neural network  [notebook1], [notebook2]\n Convolutional neural network implemented via (1) PyTorch \u0026amp; (2) Fastai  [notebook1], [notebook2]\n Re-implemented one style transfer algorithm (MSG-Net) via PyTorch  [notebook]\nAll assignments are available in *.ipynb. Best wishes!\n","date":1575072000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1578528000,"objectID":"e7dbb334fc015e3ac0d7250e4272c409","permalink":"https://yujie-he.github.io/study/2019-deep-learning/","publishdate":"2019-11-30T00:00:00Z","relpermalink":"/study/2019-deep-learning/","section":"study","summary":"TJ-100685 Ê∑±Â∫¶Â≠¶‰π† | Deep learning (Elective Course)  Supervised by Professor, Yin Wang\nGitHub Repo: hibetterheyj/tju_deep_learning\n Course info The course covers the following topics:\n Python basics Linear regression \u0026amp; Logistic regression NN basics Convolutional neural network Object detection Style transfer NLP basics Applicant: Medical image processing  Final Project Enhanced 3D Zonal Segmentation of the Prostate on MRI via Enhanced Weight-Standardization and GroupNorm\nÂü∫‰∫éÁöÑWeight-StandardizationÂíåGroupNormÁöÑ‰∏âÁª¥ÂâçÂàóËÖ∫MRIÂå∫ÂùóÂàÜÂâ≤\nüí•For more details, please refer to jupyter notebook for final project.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" TJ-100177 MATLABÂèäÂÖ∂Â∑•Á®ãÂ∫îÁî® | MATLAB and Its Applications in Engineering (Elective Course) Course info  Supervised by Professor, Yiru Dai.\n There are six assignments as of 19.06.19. The course covers the following topics:\n Matlab basic operations Numeral and Symbolic calculations Function and plot basics Simulink basics Stateflow basics Optimization toolbox  Weekly solution and codes are available in . md. If you are used to using .html or .doc(x), you can open the corresponding file.\nBest wishes!\nÊà™Ê≠¢19.06.19ÔºåÂÖ±Êúâ6Ê¨°‰Ωú‰∏öÔºåÂàÜÂà´ÂÖ≥‰∫é‰ª•‰∏ã‰∏ªÈ¢òÔºö\n Âü∫Á°ÄÊìç‰Ωú Êï∞ÂÄºÂíåÁ¨¶Âè∑ËÆ°ÁÆó ÂáΩÊï∞ÂíåÁªòÂõæÂü∫Á°Ä SimulinkÂü∫Á°Ä Stateflow Âü∫Á°Ä ‰ºòÂåñÂ∑•ÂÖ∑ÁÆ±  ÊØèÂë®Ëß£Á≠îÂíå‰ª£Á†ÅÂÖ∑‰ΩìÂèØËßÅ.md„ÄÇÂ¶Ç‰π†ÊÉØ‰ΩøÁî®.htmlÊàñÊòØ.doc(x)ÔºåÂÖ∑‰ΩìÂèØ‰ª•ÊâìÂºÄÂØπÂ∫îÊñá‰ª∂„ÄÇ\nÁ•ùÂ•ΩÔºÅ\nCode repo For more details, please refer to my repo hibetterheyj/matlab_basic/coursework in GitHubüòÄ\n","date":1561852800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1561852800,"objectID":"b5768753bcbbb224f7053f9aeb93061b","permalink":"https://yujie-he.github.io/study/2019-matlab/","publishdate":"2019-06-30T00:00:00Z","relpermalink":"/study/2019-matlab/","section":"study","summary":"TJ-100177 MATLABÂèäÂÖ∂Â∑•Á®ãÂ∫îÁî® | MATLAB and Its Applications in Engineering (Elective Course) Course info  Supervised by Professor, Yiru Dai.\n There are six assignments as of 19.06.19. The course covers the following topics:\n Matlab basic operations Numeral and Symbolic calculations Function and plot basics Simulink basics Stateflow basics Optimization toolbox  Weekly solution and codes are available in . md. If you are used to using .html or .","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" TJ-040388 Êú∫ÁîµÊ∂≤ËÆæËÆ°Âª∫Ê®° | Digital Modeling and Design of Mechanical-electrical-hydraulic System (Subject Elective Course) Course info  Supervised by Professor, Xiaotian Li\n This is a subject elective course for mechanical engineering students, which are specialized in Mechatronics. This course introduces the modeling of multi-domain engineering systems at a level of detail suitable for design and control system implementation. Topics include modeling using bond graphs, the transformation from the bond graph to block diagram, derivation of state-space models, multi-port energy storage and dissipation, and control-relevant properties.\nCode repo For more details, please refer to my repo hibetterheyj/tju_system_dynamic_modeling_coursework in GitHub üòÄ\n","date":1559520000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1559520000,"objectID":"da98bcf80ae2f64bd2f920d28c88a1be","permalink":"https://yujie-he.github.io/study/2019-system-modeling-and-design/","publishdate":"2019-06-03T00:00:00Z","relpermalink":"/study/2019-system-modeling-and-design/","section":"study","summary":"TJ-040388 Êú∫ÁîµÊ∂≤ËÆæËÆ°Âª∫Ê®° | Digital Modeling and Design of Mechanical-electrical-hydraulic System (Subject Elective Course) Course info  Supervised by Professor, Xiaotian Li\n This is a subject elective course for mechanical engineering students, which are specialized in Mechatronics. This course introduces the modeling of multi-domain engineering systems at a level of detail suitable for design and control system implementation. Topics include modeling using bond graphs, the transformation from the bond graph to block diagram, derivation of state-space models, multi-port energy storage and dissipation, and control-relevant properties.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" Final Project  üí•Online Viewer: jupyter notebook for final project\n  Title  Enhanced 3D Zonal Segmentation of the Prostate on MRI via Enhanced Weight-Standardization and GroupNorm\nÂü∫‰∫éÁöÑWeight-StandardizationÂíåGroupNormÁöÑ‰∏âÁª¥ÂâçÂàóËÖ∫MRIÂå∫ÂùóÂàÜÂâ≤\n Abstract  In this final project, I utilized Weight Standardization (WS) as well as GroupNorm to accelerate neural networks training and improve overall segmentation results for 3D Zonal Segmentation of the Prostate on MRI images. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The quantitative experiments have shown that UWG-Net can outperform the performances of 3D U-Net (baseline) with BN trained with small batch sizes with only two more lines of code. The effectiveness of WS is verified on the open-source Prostate Dataset, including 22 training cases and 10 testing cases.\n\nQuantitative experiments on Prostate Dataset. Red, green, and blue fonts denote the first, second, and third best performance among all models. \n\n","date":1578528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"453995813991317b6c45b0fdf6fd9b64","permalink":"https://yujie-he.github.io/study/2019-deep-learning/final_project/","publishdate":"2020-01-09T00:00:00Z","relpermalink":"/study/2019-deep-learning/final_project/","section":"study","summary":"Final Project  üí•Online Viewer: jupyter notebook for final project\n  Title  Enhanced 3D Zonal Segmentation of the Prostate on MRI via Enhanced Weight-Standardization and GroupNorm\nÂü∫‰∫éÁöÑWeight-StandardizationÂíåGroupNormÁöÑ‰∏âÁª¥ÂâçÂàóËÖ∫MRIÂå∫ÂùóÂàÜÂâ≤\n Abstract  In this final project, I utilized Weight Standardization (WS) as well as GroupNorm to accelerate neural networks training and improve overall segmentation results for 3D Zonal Segmentation of the Prostate on MRI images. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"  Research Assistant at Learning Algorithms and Systems Laboratory (LASA), EPFL since Oct. 2021 Supervisor: Dr. Diego Felipe Paez Granados, and Prof. Aude Billard\n Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nDemo    Qolo trajectory and tracked pedestrian in world frame   \nBackground In this work, we will create a dataset of mobile robot navigation around pedestrians from experimental data of a personal mobility device navigating autonomously around pedestrians in the streets of center Lausanne.\nThe focus will be to assess people navigation behavior around the robot by extracting trajectories and motions. I aim to build a detecting, tracking, and motion profile extraction pipeline on lidar and camera data.\n   Overview of detected pedestrian from recorded rosbag and qolo robot   \nPreliminary results  \nDataset and toolkit overview üöß To be updated!\n","date":1636156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636156800,"objectID":"a170f067d60ba70aa28d75dc76e8b379","permalink":"https://yujie-he.github.io/project/2021-qolo-pedestrian-analysis/","publishdate":"2021-11-06T00:00:00Z","relpermalink":"/project/2021-qolo-pedestrian-analysis/","section":"project","summary":"The focus will be to assess people navigation behavior around the robot by extracting trajectories and motions. I am working on a novel detecting, tracking, and motion profile extraction pipeline on lidar and camera data.","tags":["Robotics","Machine Vision","Deep Learning","ROS","LiDAR","In Progress‚ö°","Featuredüí°"],"title":"Multi-modal pedestrian behavior analysis for Qolo robot","type":"project"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Weijiang Xiong","Fan Li"],"categories":null,"content":" Main difference between the proposed ReCF and SRDCF. \n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"e00ef41dedd6f1a380d0930d4204b0d5","permalink":"https://yujie-he.github.io/publication/2021_recf_tits/","publishdate":"2021-06-29T00:00:00Z","relpermalink":"/publication/2021_recf_tits/","section":"publication","summary":"Object tracking is a fundamental task for the visual perception system on the intelligent unmanned aerial vehicle (UAV). The high efficiency of correlation filter (CF) based trackers has advanced the widespread development of online UAV object tracking. This kind of method can effectively train a filter to discriminate the target from the background. However, most CF-based methods require a fixed label function over all the previous samples, leading to over-fitting and filter degradation, especially in complex drone scenarios. To address this problem, a novel adaptive response reasoning approach is proposed for CF learning. It can leverage temporal information in filter training and significantly promote the robustness of the tracker. Specifically, the proposed response reasoning method goes beyond the standard response consistency requirement and constructs an auxiliary label of the current sample. Besides, it helps learn a generic relationship between the previous and current filters, thereby realizing self-regulated filter updating and enhancing the discriminability of the filter. Extensive experiments on four wellknown challenging UAV tracking benchmarks with 278 videos sequences show that the presented method yields superior results to 40 state-of-the-art trackers with real-time performance on a single CPU, which is suitable for UAV online tracking missions.","tags":["Real-time UAV tracking","Discriminative correlation filter","Adaptive response reasoning","Featured"],"title":"ReCF: Exploiting Response Reasoning for Correlation Filters in Real-Time UAV Tracking","type":"publication"},{"authors":null,"categories":null,"content":"  Semester Research Student at Laboratory of Intelligent Systems (LIS), EPFL since Feb. 2021 Supervisor: Valentin W√ºest (PhD student), Dr. Przemyslaw Mariusz Kornatowski, and Prof. Dario Floreano\n Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nBackground At the Laboratory of Intelligent Systems (LIS), passionate researchers are developing a human-friendly drone delivery system for last-cm delivery - Dronistics. The system is composed of a safe drone called PackDrone and software to control and monitor drones in real-time.\n --   PackDrone can eliminate the damage from propellers or rotor blades with a foldable protective cage  [Source: Dronistics]  \n What is the goal of this semester project?  Our goal is to deliver to a balcony/window which is tagged with a special symbol/pattern. Moreover, the drone should be equipped with a system of collision avoidance to prevent hitting a building.\n Why motivates us to work on this project?  One vivid example is that in this special period of Covid-19, people are required to keep social distance while delivery work keeps operating. In contrast to large aircraft, window/balcony delivery with lightweight drone is a reasonable and effective solution to send valuable parcels such as medical supplies rapidly and safely.\nSystem architecture     Illustration     Hardware    Software       System architecture of the proposed drone delivery system  \nExperiments  Visual fiducial marker evaluation  Onboard test\n  Final presentation  --  ","date":1623888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623888000,"objectID":"c8e0f9634fc10e3713030bf68a0c8a66","permalink":"https://yujie-he.github.io/project/2021-lis-drone-delivery/","publishdate":"2021-06-17T00:00:00Z","relpermalink":"/project/2021-lis-drone-delivery/","section":"project","summary":"Proposed a simplified pipeline of last-centimeter drone delivery towards window/balcony with vision-based fiducial marker detection and collision prevention under rigorous test","tags":["Robotics","Machine Vision","Mechatronics","ROS","PX4","Pixhawk","Unmanned Aerial Vehicle","Visual fiducial marker","Finishedüòä","Featuredüí°"],"title":"Development of vision based algorithms to a window/balcony drone delivery","type":"project"},{"authors":null,"categories":null,"content":"  Final project in MICRO-502 Aerial robotics\nMembers: Yujie He, Jianhao Zheng, and Longlai Qiu\nLecturer: Prof. Dario Floreano\n Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nGoal: Autonomous Navigation and Landing for Crazyflie In this practical, we programed based on Crazyflie 2.1 to find and precisely land on a platform with height of 10 cm by utilizing z reading from flow deck. Additionally, We also utilized sensor readings from multi-ranger deck to avoid the obstacles presented in the environment.\n\nPipeline    Autonomous navigation \u0026amp; landing Workflow     ‚úì Local obstacle avoidance ‚úì Grid-based coverage path planning ‚úì Waypoint following ‚úì A* search-based re-planning     Code Features  Modular library for different tasks\n‚îú‚îÄ‚îÄ cf_load_params.py # parameter setting ‚îú‚îÄ‚îÄ cf_search.py # searching functions such as, coverage planning, box edge detection, A* search ‚îú‚îÄ‚îÄ cf_state_class.py # state estimation class for the proposed task ‚îî‚îÄ‚îÄ cf_utilis.py # utility functions, such as live plotting  Utilized argparse for quick parameter adjustment and tuning\n Utilized matplotlib for real-time visualization\n  Structure  Code folder: ./code/crazyflie-lib-python/group_7/\n . ‚îú‚îÄ‚îÄ cf_load_params.py ‚îú‚îÄ‚îÄ cf_search.py ‚îú‚îÄ‚îÄ cf_state_class.py ‚îú‚îÄ‚îÄ cf_utilis.py ‚îú‚îÄ‚îÄ overall.py ‚îú‚îÄ‚îÄ draw_traj_demo.py ‚îú‚îÄ‚îÄ logs ‚îÇ ‚îú‚îÄ‚îÄ overall-20210530_1930_x.csv ‚îÇ ‚îú‚îÄ‚îÄ overall-20210530_1930_x_half.csv ‚îÇ ‚îú‚îÄ‚îÄ overall-20210530_1930_y.csv ‚îÇ ‚îî‚îÄ‚îÄ overall-20210530_1930_y_half.cs ‚îî‚îÄ‚îÄ readme.md  Demo  overall.py: overall pipeline from taking off to landing.\n# -x (float) for setting initial x position # -y (float) for setting initial y position # -v (bool) for enabling visualization python overall.py -x 0.6 -y 0.6 -v   \n draw_traj.py: x-y trajectory visualization with region annotation\n# --log_folder (str) for assigning input log folder # --logname (str) for loding log file # --img_folder (str) for assigning output image folder # -n/--name (str) for assigning output image name # --zone_anno (bool) for enabling region annotation python draw_traj_demo.py --logname overall-20210530_1930 -n cf_demo --zone_anno   \nThe estimated values drift considerably after long flights. Moreover, the predicted starting position is significantly different from the starting point after the drone re-takes off.\nExperiments    Features Figures     ‚úì Size: 480 cm (W) √ó 120 cm (H) ‚úì Starting \u0026amp; Landing pad - starting (x, y) = (60 cm, 60 cm) - landing pad randomly placed ‚úì Circular and rectangular obstacles     Video   \nSlide   \n","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"67978cde5b9a898578598f1ba3e6b0d9","permalink":"https://yujie-he.github.io/project/2021-crazyflie-auto-nav/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/project/2021-crazyflie-auto-nav/","section":"project","summary":"We programed based on [Crazyflie 2.1](https://www.bitcraze.io/products/crazyflie-2-1/) to find and precisely land on a platform with height of 10 cm by utilizing z reading from [flow deck](https://www.bitcraze.io/products/flow-deck-v2/). Additionally, We also utilized sensor readings from [multi-ranger deck](https://www.bitcraze.io/products/multi-ranger-deck/) to avoid the obstacles presented in the environment.","tags":["Robotics","Unmanned Aerial Vehicle","Finishedüòä","Featuredüí°"],"title":"Autonomous Navigation and Landing for Crazyflie","type":"project"},{"authors":[],"categories":[],"content":" Short course at EPFL  EPFL: Up to speed with Zotero  Gitbook: https://fbib.gitbooks.io/up-to-speed-with-zotero/content/ Slide: https://go.epfl.ch/ztart   Guides in Chinese  Zotero Ë∑®Âπ≥Âè∞ÂêåÊ≠•ÈôÑ‰ª∂ÁöÑÂÆûÁé∞-Zhihu Zotero ÂºÄÁÆ±ÊåáÂçó-Zhihu ZoteroÂÖ•Èó®Â∞èÊäÄÂ∑ß-Zhihu Zotero„ÄåÂ∑•ÂÖ∑ÂåÖ„ÄçÂ∑≤Â§áÂ•ΩÔºåËØ∑‰ΩøÁî®ÔºÅ-Bilibili Â¶Ç‰Ωï‰ΩøÁî®zoteroÊñáÁåÆÁÆ°ÁêÜËΩØ‰ª∂Âú®latex‰∏≠ÊéíÁâàÂèÇËÄÉÊñáÁåÆÂïäÔºü-Zhihu  Plug-ins  Better BibTeX: doc, download\nmy citekey style: [auth:lower][year][journal:abbr]\n Citation Counts Manager: download\n ZoteroQuickLook: download\n Zutilo: download\n  Useful links  Plugins for Zotero to add features: https://www.zotero.org/support/plugins\n Zotero Forum to get help from the large Zotero community: https://forums.zotero.org/discussions\n  ","date":1619064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619064000,"objectID":"85df255a67b1c2877a5aec01b4bc2235","permalink":"https://yujie-he.github.io/post/21-04-22-zotero/","publishdate":"2021-04-22T12:00:00+08:00","relpermalink":"/post/21-04-22-zotero/","section":"post","summary":"Short course at EPFL  EPFL: Up to speed with Zotero  Gitbook: https://fbib.gitbooks.io/up-to-speed-with-zotero/content/ Slide: https://go.epfl.ch/ztart   Guides in Chinese  Zotero Ë∑®Âπ≥Âè∞ÂêåÊ≠•ÈôÑ‰ª∂ÁöÑÂÆûÁé∞-Zhihu Zotero ÂºÄÁÆ±ÊåáÂçó-Zhihu ZoteroÂÖ•Èó®Â∞èÊäÄÂ∑ß-Zhihu Zotero„ÄåÂ∑•ÂÖ∑ÂåÖ„ÄçÂ∑≤Â§áÂ•ΩÔºåËØ∑‰ΩøÁî®ÔºÅ-Bilibili Â¶Ç‰Ωï‰ΩøÁî®zoteroÊñáÁåÆÁÆ°ÁêÜËΩØ‰ª∂Âú®latex‰∏≠ÊéíÁâàÂèÇËÄÉÊñáÁåÆÂïäÔºü-Zhihu  Plug-ins  Better BibTeX: doc, download\nmy citekey style: [auth:lower][year][journal:abbr]\n Citation Counts Manager: download\n ZoteroQuickLook: download\n Zutilo: download\n  Useful links  Plugins for Zotero to add features: https://www.zotero.org/support/plugins\n Zotero Forum to get help from the large Zotero community: https://forums.","tags":["zotero","reference-management"],"title":"2021-04-22: Zotero for Reference Management","type":"post"},{"authors":null,"categories":null,"content":"  MICRO-453 Robotics practicals, EPFL, 2021 Spring\nStudents: Chuanfang Ning, Jianhao Zheng, and Yujie He\n Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nüîë Keywords Thymio, PID, Way following, Obstacle avoidance, Pledge algorithm, ArUco marker, Sim2Real, Gazebo\n The tested environment in Gazebo and real-world setup \nüì∑ Example videos  For more examples, please refer to https://go.epfl.ch/ros_basics_final_2021.\n    Simulation in Gazebo Real-world test on campus          üî® How to launch the example code?  For more details, please refer to hibetterheyj/EPFL_ROS_Practicals_Project.\n  simulate Thymio robot in Gazebo with an interactive window  roslaunch ros_basics_control simu_thymio.launch   adding waypoints and obstacle for the robot  roslaunch ros_basics_control simu_thymio.launch roslaunch ros_basics_exercise set_simu_waypoints_obstacle.launch   extract pose and sensor information from rosbag files  roslaunch ros_basics_exercise view_with_rosbag.launch # open a new terminal rosrun ros_basics_exercise topic_reader.py   plot trajectory comparison between real and simulation (using matlab)  cd results_from_bag/ # run `plot_traj_comp.m` in MATLAB  ‚≠êÔ∏è Acknowledgement Thanks to Vaios Papaspyros and Rafael Barmak from MOBOTS at EPFL for the amazing course tutorials !\n","date":1616371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616371200,"objectID":"9dd21fc2da0e09a748ce90ed2dc2e900","permalink":"https://yujie-he.github.io/project/2021-ros-basics/","publishdate":"2021-03-22T00:00:00Z","relpermalink":"/project/2021-ros-basics/","section":"project","summary":"Implemented autonomous navigation with obstacle avoidance of the Thymio-II robot from simulation in Gazebo to real-world tests","tags":["Robotics","ROS","Thymio","Sim2Real","Finishedüòä"],"title":"Sim2Real Development for Thymio with ROS","type":"project"},{"authors":["Yue Pan","Pengchuan Xiao","Yujie He","Zhenlei Shao","Zesong Li"],"categories":null,"content":"  Pipeline of the multi-metric linear least square ICP \nDemo  Video  \n \nKITTI results   \nReference If you find this project is useful, you may cite it as:\n@inproceedings{Pan2021ICRA, title={{MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square}}, author={Pan, Yue and Xiao, Pengchuan and He, Yujie and Shao, Zhenlei and Li, Zesong}, booktitle={Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)}, year={2021}, pages={1-8} }  ","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"2ac8cd3c09dfdba8af9f6cc3ff4e27fb","permalink":"https://yujie-he.github.io/publication/2021_mulls_icra/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/publication/2021_mulls_icra/","section":"publication","summary":"The rapid development of autonomous driving and mobile mapping calls for off-the-shelf LiDAR SLAM solutions that are adaptive to LiDARs of different specifications on various complex scenarios. To this end, we propose MULLS, an efficient, low-drift, and versatile 3D LiDAR SLAM system. For the front-end, roughly classified feature points (ground, facade, pillar, beam, etc.) are extracted from each frame using dual-threshold ground filtering and principal components analysis. Then the registration between the current frame and the local submap is accomplished efficiently by the proposed multi-metric linear least square iterative closest point algorithm. Point-to-point (plane, line) error metrics within each point class are jointly optimized with a linear approximation to estimate the ego-motion.  Static feature points of the registered frame are appended into the local map to keep it updated. For the back-end, hierarchical pose graph optimization is conducted among regularly stored history submaps to reduce the drift resulting from dead reckoning. Extensive experiments are carried out on three datasets with more than 100,000 frames collected by six types of LiDAR on various outdoor and indoor scenarios. On the KITTI benchmark, MULLS ranks among the top LiDAR-only SLAM systems with real-time performance.","tags":["LiDAR","Autonomous driving","Linear least square","Simultaneous localization and mapping","Featured"],"title":"MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square","type":"publication"},{"authors":["Changhong Fu","Junjie Ye","Juntao Xu","Yujie He","Fuling Lin"],"categories":null,"content":"![BiCF_comp](featured.png) Comparison between discriminative correlation filter (DCF) and the proposed BiCF  ### Reference If you find this project is useful, you may cite it as: ```tex @article{Lin2020TCSVT, title={{Learning Temporary Block-Based Bidirectional Incongruity-Aware Correlation Filters for Efficient UAV Object Tracking}}, author={Lin, Fuling and Fu, Changhong and He, Yujie and Guo, Fuyu and Tang, Qian}, booktitle={IEEE Transactions on Circuits and Systems for Video Technology}, pages={1-14}, year={2020} } ``` -- ","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"299526b79eb37f225827520901764903","permalink":"https://yujie-he.github.io/publication/2020-ibri-tgrs/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/publication/2020-ibri-tgrs/","section":"publication","summary":"Aerial object tracking approaches based on discriminative correlation filter (DCF) have attracted wide attention in the tracking community due to their impressive progress recently. Many studies introduce temporal regularization into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. However, existing temporal regularization approaches usually utilize the information of two consecutive frames, which are not robust enough due to limited information. Although some methods attempt to incorporate abundant training samples and generally improve the tracking performance, these improvements are at the expense of significantly increased computing consumption. Besides, most existing methods introduce historical information directly without denoising, which means background noises are also introduced into the filter training and may degrade the tracking accuracy. To tackle the drawbacks mentioned above, this work proposes a novel aerial object tracking approach to exploit disruptor-aware interval-based response inconsistency, i.e., IBRI tracker. The proposed method is able to incorporate historical interval information by utilizing responses in the filter training process, thereby obtaining a robust tracking performance while maintaining the real-time speed. Moreover, to reduce the disruptions caused by similar object, partial occlusion, and other challenging scenes, a novel disruptor-aware scheme based on response bucketing is introduced to detect the disruptor and enforce a spatial penalty for the disruptive area around the tracked object. Exhausted experiments on multiple well-known challenging aerial tracking benchmarks demonstrate the accuracy and robustness of the proposed IBRI tracker against other 35 state-of-the-art trackers. With a real-time speed of ~32 frames per second on a single CPU, the proposed approach can be applied for typical aerial platforms to achieve aerial visual object tracking efficiently.","tags":["Aerial object tracking","Discriminative correlation filter","Interval-based response inconsistency","Disruptor-aware bucketing"],"title":"Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking","type":"publication"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":" ![BiCF_comp](featured.png) Comparison between discriminative correlation filter (DCF) and the proposed BiCF  -- Reference If you find this project is useful, you may cite it as:\n@article{Lin2020TCSVT, title={{Learning Temporary Block-Based Bidirectional Incongruity-Aware Correlation Filters for Efficient UAV Object Tracking}}, author={Lin, Fuling and Fu, Changhong and He, Yujie and Guo, Fuyu and Tang, Qian}, booktitle={IEEE Transactions on Circuits and Systems for Video Technology}, pages={1-14}, year={2020} }  ","date":1599782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599782400,"objectID":"3b9988aa77574382216f4b32b3877f70","permalink":"https://yujie-he.github.io/publication/2020-tbbicf-tcsvt/","publishdate":"2020-09-11T00:00:00Z","relpermalink":"/publication/2020-tbbicf-tcsvt/","section":"publication","summary":"In the field of UAV object tracking, correlation filter based approaches have received lots of attention due to their computational efficiency. The methods learn filters by the ridge regression and generate response maps to distinguish the specified target from the background. An ideal filter can predict the object‚Äôs position in a new frame, and in turn, can backtrack the object in the past frames. However, the neglect of tracking reversibility in most methods limits the potential of using inter-frame information to improve performance. In this work, a novel bidirectional incongruity-aware correlation filter is presented based on the nature of tracking reversibility. The proposed method incorporates the response-based bidirectional incongruity, which represents the gap between the filters‚Äô discriminative difference in the forward and backward tracking perspective caused by object appearance changes. It enables the filter not only to inherit the discriminability from previous filters but also to enhance the generalization capability to unpredictable appearance variations in upcoming frames. Moreover, a temporary block-based strategy is introduced to empower the filter accommodate more drastic object appearance changes and make more effective use of inter-frame information. Comprehensive experiments are conducted on three challenging UAV tracking benchmarks, including UAV123@10fps, DTB70, and UAVDT. Experimental results indicate that the proposed method has superior performance compared with the other 34 state-of-the-art trackers. Our approach permits real-time performance at ~46.8 FPS on a single CPU and is suitable for UAV online tracking applications.","tags":["Aerial video analysis","Unmanned aerial vehicles","Visual object tracking","Discriminative correlation filter","Temporary block-based bidirectional incongruity"],"title":"Learning Temporary Block-Based Bidirectional Incongruity-Aware Correlation Filters for Efficient UAV Object Tracking","type":"publication"},{"authors":["Yujie He","Changhong Fu","Fuling Lin","Yiming Li","Peng Lu"],"categories":null,"content":" Comparison between baseline KCC tracker and the proposed TACF tracker -- Videos  Presentation  \n \n Tracking results demo  \n \nReference If you find this project is useful, you may cite it as:\n@inproceedings{He2020IROS, title={{Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters}}, author={He, Yujie and Fu, Changhong and Lin, Fuling and Li, Yiming and Lu, Peng}, booktitle={Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)}, year={2020}, pages={1575-1582} }  ","date":1599004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599004800,"objectID":"47a2a3b4e72a5eb5b1a9a8d684e77efb","permalink":"https://yujie-he.github.io/publication/2020_tacf_iros/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/2020_tacf_iros/","section":"publication","summary":"Object tracking has been broadly applied in unmanned aerial vehicle (UAV) tasks in recent years. However, existing algorithms still face difficulties such as partial occlusion, clutter background, and other challenging visual factors. Inspired by the cutting-edge attention mechanisms, a novel object tracking framework is proposed to leverage multi-level visual attention. Three primary attention, i.e., contextual attention, dimensional attention, and spatiotemporal attention, are integrated into the training and detection stages of correlation filter-based tracking pipeline. Therefore, the proposed tracker is equipped with robust discriminative power against challenging factors while maintaining high operational efficiency in UAV scenarios. Quantitative and qualitative experiments on two well-known benchmarks with 173 challenging UAV video sequences demonstrate the effectiveness of the proposed framework. The proposed tracking algorithm favorably outperforms 12 state-of-the-art methods, yielding 4.8% relative gain in UAVDT and 8.2% relative gain in UAV123@10fps against the baseline tracker while operating at the speed of ‚àº28 frames per second.","tags":["Visual tracking","Unmanned aerial vehicles","Attention mechanisms","Featured"],"title":"Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters","type":"publication"},{"authors":null,"categories":null,"content":"  Undergraduate Research Assistant since Sep. 2018\n Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nOverview Investigated correlation filter (CF)-based visual object tracking for unmanned aerial vehicles. By applying machine learning \u0026amp; deep learning techniques, we have improved the existing trackers on overall tracking performance in challenging scenarios with real-time operational capability.\nPapers with code Related work has been published in journals and conferences as follows:\n Proposed a lightweight and generalizable triple attention strategy on CF-based framework by exploiting mutual independence of the appearance model and feature responses to implement real-time tracking for UAV.\nüö© Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters in IROS 2020\n Employed the adaptive GMSD-based context analysis and dynamic weighted filters for utilizing both contextual and historical information, and leveraged lightweight convolution features to efficiently raise the tracking robustness.\nüö© Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters in Neural Computing and Applications\n Exploited the inter-frame information between prediction and backtracking phases for further incorporating the bidirectional incongruity error into the CF learning.\nüö© BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking in ICRA 2020\n   For more info, please refer to my YouTube channel and GitHub.   ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"67278111707c92b4a03715b9e16fdc2b","permalink":"https://yujie-he.github.io/project/2020-tracking4uav/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/2020-tracking4uav/","section":"project","summary":"Improved the existing trackers on overall performance in challenging UAV scenarios with high operational efficiency","tags":["Robotics","Machine Vision","Deep Learning","Unmanned aerial vehicle","Finishedüòä","Featuredüí°"],"title":"Online Visual Object Tracking for UAV in Dynamic Environments","type":"project"},{"authors":["Fuling Lin","Changhong Fu","Yujie He","Fuyu Guo","Qian Tang"],"categories":null,"content":" \nComparison between discriminative correlation filter (DCF) and the proposed BiCF\n\nReference If you find this project is useful, you may cite it as:\n@inproceedings{Lin2020ICRA, title={{BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking}}, author={Lin, Fuling and Fu, Changhong and He, Yujie and Guo, Fuyu and Tang, Qian}, booktitle={Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)}, pages={2365-2371}, year={2020} }  ","date":1582848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582848000,"objectID":"efc7d26999d0ac3d6a28bb149db67779","permalink":"https://yujie-he.github.io/publication/2020_bicf_icra/","publishdate":"2020-02-28T00:00:00Z","relpermalink":"/publication/2020_bicf_icra/","section":"publication","summary":"Correlation Ô¨Ålters have shown excellent performance in unmanned aerial vehicle (UAV) tracking scenarios due to their high computational efÔ¨Åciency. During the UAV tracking process, viewpoint variations are usually accompanied by changes in the object and background appearance, which poses a unique challenge to CF-based trackers. Since the appearance is gradually changing over time, an ideal tracker can not only forward predict the object position but also backtrack to locate its position in the previous frame. There exist errors in the reversibility of the tracking process, which contains the information on the changes in appearance. However, some existing methods do not consider the forward and backward errors while using only the current training sample to learn the Ô¨Ålter. For other ones, the applicants of considerable historical training samples impose a computational burden on the UAV. In this work, a novel bidirectional incongruity-aware correlation Ô¨Ålter, i.e., BiCF tracker, is proposed. By integrating the bidirectional incongruity error into the CF, BiCF can efÔ¨Åciently learn the changes in appearance and suppress the inconsistent error. Extensive experiments on 243 challenging image sequences from three UAV datasets (i.e., UAV123, UAVDT, and DTB70) are conducted to demonstrate that the proposed BiCF tracker favorably outperforms other 25 state-of-the-art trackers and achieves a real-time speed of 45.4 FPS on a single CPU, which can be applied in UAV efÔ¨Åciently.","tags":["Visual tracking","Unmanned aerial vehicles"],"title":"BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking","type":"publication"},{"authors":null,"categories":null,"content":" Final project for TJ-100685 Ê∑±Â∫¶Â≠¶‰π† | Deep learning, Sep. 2019 - Jan. 2020\n  Utilized the latest Weight Standardization}** (WS) as well as GroupNorm to accelerate neural networks training from scratch for 3D Zonal Segmentation of the Prostate MRI images.\n Conducted extensive evaluation between the proposed UWG-Net with the baseline with small batch sizes, achieving 2-3\\% increase in multi-class segmentation accuracy for medical imaging application.\n  For more info, you can refer to this page !\n","date":1578355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578355200,"objectID":"c0a3faf6ff114a1520a3feced6e21559","permalink":"https://yujie-he.github.io/project/2019-dl-mip/","publishdate":"2020-01-07T00:00:00Z","relpermalink":"/project/2019-dl-mip/","section":"project","summary":"Implemented 3D Segmentation on medical images and got A in deep learning elective course.","tags":["Machine Vision","Deep Learning","Finishedüòä"],"title":"3D Zonal Segmentation of the Prostate MRI (Deep Learning Final Project)","type":"project"},{"authors":["Changhong Fu","Yujie He","Fuling Lin","Weijiang Xiong"],"categories":null,"content":" \nMain structure of the proposed tracking approach\n\nReference If you find this project is useful, you may cite it as:\n@article{Fu2020NCAA, title={{Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters}}, author={Fu, Changhong and He, Yujie and Lin, Fuling and Xiong, Weijiang}, journal={Neural Computing and Applications}, pages={1-17}, year={2020} }  ","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575504000,"objectID":"3a099303b9e3dae061a1cf46fc4af39e","permalink":"https://yujie-he.github.io/publication/2020_mkct_ncaa/","publishdate":"2020-01-31T00:00:00Z","relpermalink":"/publication/2020_mkct_ncaa/","section":"publication","summary":"In recent years, the correlation filter (CF)-based method has significantly advanced in the tracking for unmanned aerial vehicles (UAVs). As the core component of most trackers, CF is a discriminative classifier to distinguish the object from the surrounding environment. However, the poor representation of the object and lack of contextual information have restricted the tracker to gain better performance. In this work, a robust framework with multi-kernelized correlators is proposed to improve robustness and accuracy simultaneously. Both convolutional features extracted from the neural network and handcrafted features are employed to enhance expressions for object appearances. Then, the adaptive context analysis strategy helps filters to effectively learn the surrounding information by introducing context patches with the GMSD index. In the training stage, multiple dynamic filters with time-attenuated factors are introduced to avoid tracking failure caused by dramatic appearance changes. The response maps corresponding to different features are finally fused before the novel resolution enhancement operation to increase distinguishing capability. As a result, the optimization problem is reformulated, and a closed-form solution for the proposed framework can be obtained in the kernel space. Extensive experiments on 100 challenging UAV tracking sequences demonstrate the proposed tracker outperforms other 23 state-of-the-art trackers and can effectively handle unexpected appearance variations under the complex and constantly changing working conditions.","tags":["Visual tracking","Unmanned aerial vehicles","Multi-kernelized correlators","Adaptive context analysis","Dynamic weighted filters","Featured"],"title":"Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters","type":"publication"},{"authors":[],"categories":[],"content":"It\u0026rsquo;s my first time to participate the top robotics conference\u0026ndash;IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2019) held on Nov. 4 ‚Äì 8, 2019 in Macau, China.\nFeel very lucky to talk to the experts, professors, and my fellows!\nOur paper \u0026lsquo;Sample Purification-Aware Correlation Filters for UAV Tracking with Cooperative Deep Features\u0026rsquo; is accepted by the Workshop on Fast Neural Perception and Learning for Intelligent Vehicles and Robotics and won the Best Poster Award!\nFor more details, please refer to SPCF!\n\nMy first time to attend top robotics conference, IROS @ Macau\n\nKeynote by *Davide Scaramuzza* covering Autonomous Drones Event Cameras  -- ","date":1572840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572840000,"objectID":"b06a9c527d705b63f1d8f86cf734ca2c","permalink":"https://yujie-he.github.io/post/19-11-iros/","publishdate":"2019-11-04T12:00:00+08:00","relpermalink":"/post/19-11-iros/","section":"post","summary":"It\u0026rsquo;s my first time to participate the top robotics conference\u0026ndash;IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2019) held on Nov. 4 ‚Äì 8, 2019 in Macau, China.\nFeel very lucky to talk to the experts, professors, and my fellows!\nOur paper \u0026lsquo;Sample Purification-Aware Correlation Filters for UAV Tracking with Cooperative Deep Features\u0026rsquo; is accepted by the Workshop on Fast Neural Perception and Learning for Intelligent Vehicles and Robotics and won the Best Poster Award!","tags":["conference","IROS"],"title":"2019-11-04: Our paper SPCF won the Best Poster Award in the IROS Workshop, Macua, China.","type":"post"},{"authors":["Changhong Fu","Fuling Lin","Fan Li","Yujie He"],"categories":null,"content":"\nComparison between traditional CF-based and proposed SPCF tracker\n\n","date":1567209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567209600,"objectID":"885aa35a9654e3a8444e0e183be373c5","permalink":"https://yujie-he.github.io/publication/2019_spcf_irosw/","publishdate":"2019-08-31T00:00:00Z","relpermalink":"/publication/2019_spcf_irosw/","section":"publication","summary":"Correlation Filters (CF) have recently demonstrated promising performance in terms of rapidly tracking objects for unmanned aerial vehicles (UAV) in different types of UAV tracking tasks. The strength of the approach comes from its ability to learn how the object is changing over time efficiently. However, due to heavy dependence on the quality of the training set and the lack of real negative training samples, the object appearance model may be easily interfered by the corrupted training samples, which can result in suboptimal performance. Besides, limited by the representation of a single feature, the tracking model may fail to cope with the complex surrounding environment and considerable appearance variation of the object. In this work, the principal causes behind the problems of the abundance of object representation and purity of training samples have been tackled, to simultaneously improve both discriminative power and anti-disturbance capability of the tracking model. Comprehensive experiments on 100 challenging UAV image sequences have demonstrated that the novel sample purifying tracker based on cooperative features learning, i.e., SPCF tracker, outperforms 15 state-of-the-art trackers in terms of efficiency, robustness, and accuracy. To the best of our knowledge, the presented SPCF tracker is designed for object tracking and employed in UAV tracking tasks for the first time.","tags":["Correlation filter","Unmanned aerial vehicles","Sample purification"],"title":"Sample Purification-Aware Correlation Filters for UAV Tracking with Cooperative Deep Features","type":"publication"},{"authors":null,"categories":null,"content":" Teaching Assistant, Sep. 2018 - Jan. 2019\n  Assisted first-year students major in Industrial Design to get started Arduino Hardware and Programming\n Designed a series of the electromechanical modules for Industrial Design students\n Gave lectures on basic mechanical principle with Arduino hardware and programming and advanced RGBD sensors for the semester project\n  The following video is Mechatronics Module Experiments.\n\n \n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"5fb85218b7c762be752eebb403211af0","permalink":"https://yujie-he.github.io/project/2019-tongji-ta/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/2019-tongji-ta/","section":"project","summary":"Acted as TA for freshmen in [College of Design \u0026 Innovation](http://tjdi.tongji.edu.cn/)","tags":["Mechatronics","Finishedüòä"],"title":"Teaching Assistant in Open Source Hardware and Programming","type":"project"},{"authors":null,"categories":null,"content":" Powertrain Group Leader, Sep. 2016 - Dec. 2018\n  Designed and optimized the overall powertrain system to ensure China\u0026rsquo;s first leading four-wheel-drive Formula Student Racecar, achieving an 8\\% efficiency and 10\\% lightweight improvement Participated FSEC 2017 - 2018 and SFJ 2018 as Chief Powertrain Engineer, contributing to DIAN Racing‚Äòs win in first place in Engineering Design and Efficiency Prize, and Best Powertrain Award from 2017 to 2018  The following video is virtual assembly of DRe18Ôºåwhich was what I worked for as Powertrain Group Leader.\n\n  Design Report Final @ FSEC19 \n","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546214400,"objectID":"82d14568390f43b7be1a421e74f001ca","permalink":"https://yujie-he.github.io/project/2018-dian-racing/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/project/2018-dian-racing/","section":"project","summary":"Built First leading 4WD Formula Student Racecar in China.","tags":["Mechatronics","Racecar","Finishedüòä"],"title":"DIAN Racing Formula Student Electric Team","type":"project"},{"authors":null,"categories":null,"content":"  Robotics Algorithm Development Intern, Jul. 2018 - Aug. 2018\n Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nOverview  Implemented sensor fusion between a 40-channel LiDAR, i.e., Pandar40 and gyroscope and achieved a 5% accuracy improvements on top of state-of-the-art SLAM framework and drew a 3D point cloud map of Tongji University Jiading Campus below 10m Deployed control, decision, and communication algorithms for a self-developed skid steer wheel robot, realizing autonomous navigation and obstacle avoidance in a $ 300 m^2 $ workspace  Demos Examples of final mapping results can be seen as follows:\n\n Pointcloud Demo of Tongji Jiading Campus üëâ Corresponding satellite map from Google map Up: Main Gate, Down: Kaiwu Building \n\nMisc. \n Indoor SLAM @ Hesai Tech\n Skid Steer Wheel Robot equipped with Pandar40 LiDAR\n\n","date":1535673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535673600,"objectID":"2cf2900a21859645e66bc3bb95d82458","permalink":"https://yujie-he.github.io/project/2018-hesai-internship/","publishdate":"2018-08-31T00:00:00Z","relpermalink":"/project/2018-hesai-internship/","section":"project","summary":"Implemented Outdoor SLAM @ Tongji Jiading Campus and Indoor Autonomous Navigation.","tags":["Robotics","Machine Vision","LiDAR","Finishedüòä"],"title":"SLAM and Autonomous Navigation for Skid Steer Wheel Robot","type":"project"},{"authors":null,"categories":null,"content":" Project Manager \u0026amp; Mechanical Development Leader, Oct. 2016 - Jun. 2018\n  Designed two main robots to participate in national mobile robot competition, RoboMaster, achieving lightweight and stability of the chassis and 3DOF pan-tilt mechanism and multi-robot interaction Optimized structural design to enhance operation stability and achieve lightweight, enable the robots flexible operation and combating under complicated circumstances  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"26fb77baca14eeca774e060f789d6ff1","permalink":"https://yujie-he.github.io/project/2018-super-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/2018-super-power/","section":"project","summary":"Designed robots to combat in RoboMaster.","tags":["Mechatronics","Robotics","Finishedüòä"],"title":"Super Power Robot Team","type":"project"}]